#!/usr/bin/env python3


"""

Manager, also called proxy. Connects to rabbitmq to accept jobs. Declares remoulade actors (which follow remoulade semantics, returning results through postgres, etc), and relays them to workers. There are three deployment scenarios that we'll try to support:

## trusted workers in docker stack
id is generated by docker

## untrusted workers in fly.io machines
id and auth token generated in manager.

## untrusted workers connecting over public internet
These would be set up by user in frontend, id and auth token would be generated for each.



Manager has two outer-facing parts:
	* the api, which workers connect to
	* remoulade_thread, the remoulade actors, and the connection to rabbitmq

parallelization:
	* manager could be deployed across multiple containers, or uvicorn could spawn multiple worker processes. In both cases, care would have to be taken that a given (robust) worker would always connect to the same manager process (balancer affinity).	


"""



import logging
import os, sys
import threading
import time, asyncio
from typing import Optional

sys.path.append(os.path.normpath(os.path.join(os.path.dirname(__file__), '../../common/libs/misc')))
from tasking import remoulade

from fastapi import FastAPI, Request

from app.isolated_worker import *
from app.untrusted_task import *
import app.manager_actors



log = logging.getLogger(__name__)
log.setLevel(logging.DEBUG)
log.addHandler(logging.StreamHandler(sys.stderr))

loop_log = logging.getLogger(__name__)
loop_log.setLevel(logging.INFO)
loop_log.addHandler(logging.StreamHandler(sys.stderr))

logging.basicConfig()
#logging.config.fileConfig('logging.yaml', defaults=None, disable_existing_loggers=False, encoding=None)



app = FastAPI(
	title="Robust API",
	summary="invoke accounting calculators and other endpoints",
)


@app.get("/")
async def read_root():
	return {"Hello": "World"}


@app.get("/health")
async def read_root():
	return "ok"


@app.post("/worker/{worker_id}/heartbeat")
def post_heartbeat(worker_id: str, task_id: str = None):
	"""
	While worker is processing a task, it should take care to call /worker/{id}/heartbeat every minute. - it can also do this the whole time, even when there's no task.
	"""
	log.debug('heartbeat %s %s', worker_id, task_id)
	worker = get_worker(worker_id, last_seen=datetime.datetime.now())
	worker.last_reported_task = task_id
	worker.last_reported_task_ts = datetime.datetime.now()



@app.post("/worker/{worker_id}/messages")
async def post_messages(request: Request, worker_id: str, inmsg: dict):
	"""
	Hangs until a message is available. Worker calls this in a loop.
	
	Always at most one message in the queue. The messages are:
		"ping" - worker should respond with "pong"
		a task - worker should start processing the task, and when it's done submit task_result in next call to /worker/{id}/messages
	
	If the manager has been reset while worker is processing a task:
		* this task_result will be ignored, worker will go on to the next task.
		* the remoulade task will eventually get marked as failed.
	
	IF the worker has been reset while processing a task:
		manager detects this because it expects a task_result, but gets none. The remoulade task will be made to return with failure.
		
	It might be possible that a client issues two requests to /messages with some overlap. This might happen if the connection breaks or client disconnects, and immediately connects again (as it should), but the first request is still waiting on toworker.get(1).
	In this case, the message will be lost. This is the same as if the worker never connected back again.
	
	 Concievably, the events pushed here can be pushed multiple times, the client can invoke this multiple times, if a connection error occurs during handling
	 
	"""

	task_result = inmsg.get('task_result')
	worker_info = inmsg.get('worker_info')


	log.debug('')
	log.debug('')
	log.info('/messages worker_id=%s task_result=%s', worker_id, task_result)

	worker = get_worker(worker_id, last_seen=datetime.datetime.now())

	outmsg = {}

	if task_result:
		outmsg |= on_task_result(worker=worker, result=task_result)
	else:
		if worker.task and worker.task_given_ts:
			time_since_task_sent_to_worker = datetime.datetime.now() - worker.task_given_ts
			#log.debug(f'{time_since_task_sent_to_worker=}')
			if time_since_task_sent_to_worker > datetime.timedelta(seconds=15):
				# grace period, because in the loop below, we may think that we sent a response with task, but the worker might have been already disconnected. But we only record task_given_ts the first time we relay the task, so, if a worker keeps disconnecting, we eventually ...do...something?
				log.warn(f"""{worker.id} should be working on {worker.
						 task_id} and sending heartbeats, but it's coming back without result... {time_since_task_sent_to_worker=}""")
				put_event(dict(type='worker_died', worker=worker))
		else:
			put_event(dict(type='worker_available', worker=worker))
			# give synchronization_thread some time to assign task to worker. todo this doesn't need to happen in a separate thread, can happen synchronously.
			await asyncio.sleep(2)

	counter = 0
	log.debug(f'begin loop {worker_id=}')
	
	while not await request.is_disconnected():
	
		log.debug(f'begin loop2 {worker_id=} {counter=}')
		counter += 1


		heartbeat(worker)
		log.debug(f'begin loop3 {worker_id=}')
		
		

		#log.debug('id(workers)=%s', id(workers))
		log.info(f'{len(workers)=}')
		#for _,v in workers.items():
		#	log.debug('worker %s', v)
		#log.debug('worker %s not disconnected', worker_id)


		log.info(f'{len(pending_tasks)=}')
		for v in pending_tasks:
			log.debug('%s', v)


		if worker.task:
			log.debug('give task: %s', worker.task)
			if not worker.task_given_ts:
				worker.task_given_ts = datetime.datetime.now()
			return outmsg | dict(task=dict(id = worker.task.id, proc=worker.task.proc, args=worker.task.args, worker_options=worker.task.worker_options))


		log.debug('sleep')
		log.debug('')
		try:
			#asyncio.create_task(handler_wakeup_wait_timeout, name='MyTask')
			await asyncio.wait_for(worker.handler_wakeup.wait(), timeout=10)
			#await asyncio.sleep(5)
		except asyncio.exceptions.TimeoutError:
			pass
		

		# give some time for actors to relay the result, but then let's not keep the worker hanging around with result indefinitely, because it always timeouts first, and we never actually send him a message. So let's send him the ack.

		if not worker.task and task_result:
			log.debug(str(outmsg))
			return outmsg


	log.info('hangup %s', worker_id)
	log.debug('')



# async def handler_wakeup_wait_timeout(handler_wakeup):
# 	log.debug('handler_wakeup_wait_timeout')
# 	try:
# 		await asyncio.wait_for(handler_wakeup.wait(), timeout=10)
# 	except asyncio.TimeoutError:
# 		pass



def remoulade_thread():
	"""
	this is a copy of remoulade.__main__.start_worker that works inside a thread.
	Spawn a remoulade worker and periodically check if it's still running.
	"""
	logger = logging.getLogger('remoulade')

	broker = remoulade.get_broker()
	broker.emit_after("process_boot")

	# i'm afraid there isnt a good way to healthcheck manager in the same way that we healthcheck the (trusted) workers container. I mean, we could run two manager processes, and devise one worker to serve it....idk
	worker = remoulade.Worker(broker, queues=[os.environ['QUEUE']], worker_threads=int(os.environ.get('WORKER_PROCESSES',300))*2, prefetch_multiplier=1)
	worker.start()

	running = True
	while running and not shutdown_event.is_set():
		if worker.consumer_stopped or worker.worker_stopped:
			running = False
			logger.info("Worker thread is not running anymore, stopping Worker.")
		else:
			time.sleep(1)

	worker.stop(5 * 1000)
	broker.emit_before("process_stop")
	broker.close()



print(threading.Thread(target=remoulade_thread, daemon=True).start())
print(threading.Thread(target=synchronization_thread, daemon=True).start())




# @app.on_event("startup")
# async def startup_event():
#     # Start the background thread
#     threading.Thread(target=background_task, daemon=True).start()

@app.on_event("shutdown")
async def shutdown():
	shutdown_event.set()
	events.put(dict(type='nop'))
	

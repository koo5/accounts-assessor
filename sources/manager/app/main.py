#!/usr/bin/env python3


"""
there are three types of workers, three modalities that we'll try to support:

## trusted workers in docker stack
id is generated by docker

## untrusted workers in fly.io machines
id and auth token generated in manager.

## untrusted workers connecting over public internet
These would be set up by user in frontend, id and auth token would be generated for each.

Any metadata like "org" can be looked up from the id.



"""



import logging
import os, sys
import threading
import time

sys.path.append(os.path.normpath(os.path.join(os.path.dirname(__file__), '../../common/libs/misc')))
from tasking import remoulade

from fastapi import FastAPI, Request


from app.isolated_worker import *
from app.untrusted_task import *
import app.manager_actors


log = logging.getLogger(__name__)
log.setLevel(logging.DEBUG)
log.addHandler(logging.StreamHandler(sys.stderr))



app = FastAPI(
	title="Robust API",
	summary="invoke accounting calculators and other endpoints",
)


@app.get("/", include_in_schema=False)
async def read_root():
	"""	hello world	"""
	return {"Hello": "World"}



@app.post("/worker/{worker_id}/heartbeat")
def post_heartbeat(worker_id: str, task_id: str = None):
	"""
	While worker is processing a task, it should take care to call /worker/{id}/heartbeat every minute. - it can also do this the whole time, even when there's no task.
	"""
	log.debug('heartbeat %s %s', worker_id, task_id)
	worker = get_worker(worker_id, last_seen=datetime.datetime.now())
	worker.last_reported_task = task_id
	worker.last_reported_task_ts = datetime.datetime.now()



@app.post("/worker/{worker_id}/messages")
async def post_messages(request: Request, worker_id: str, task_result=None, worker_info=None):
	"""
	Hangs until a message is available. Worker calls this in a loop.
	
	Always at most one message in the queue. The messages are:
		"ping" - worker should respond with "pong"
		a task - worker should start processing the task, and when it's done submit task_result in next call to /worker/{id}/messages
	
	If the manager has been reset while worker is processing a task:
		* this task_result will be ignored, worker will go on to the next task.
		* the remoulade task will eventually get marked as failed.
	
	IF the worker has been reset while processing a task:
		manager detects this because it expects a task_result, but gets none. The remoulade task will be made to return with failure.
		
	It might be possible that a client issues two requests to /messages with some overlap. This might happen if the connection breaks or client disconnects, and immediately connects again (as it should), but the first request is still waiting on toworker.get(1).
	In this case, the message will be lost. This is the same as if the worker never connected back again.
	
	 Concievably, the events pushed here can be pushed multiple times, the client can invoke this multiple times, if a connection error occurs during handling
	 
	"""
	log.debug('messages worker_id=%s task_result=%s', worker_id, task_result)
	
	worker = get_worker(worker_id, last_seen=datetime.datetime.now())
	
	if task_result:
		put_event(dict(type='task_result', worker=worker, result=task_result))
	
	while not await request.is_disconnected():
		log.debug('worker %s not disconnected', worker_id)
		heartbeat(worker)
		if worker.task:
			log.debug('messages: worker.task: %s', worker.task)
			return worker.task
		log.debug('sleep: %s', worker.task)
		time.sleep(1)





def remoulade_thread():
	"""
	this is a copy of remoulade.__main__.start_worker that works inside a thread.
	Spawn a remoulade worker and periodically check if it's still running.
	"""
	logger = logging.getLogger('remoulade')

	broker = remoulade.get_broker()
	broker.emit_after("process_boot")

	# i'm afraid there isnt a good way to healthcheck manager in the same way that we healthcheck the (trusted) workers container. I mean, we could run two manager processes, and devise one worker to serve it....idk
	worker = remoulade.Worker(broker, queues=[os.environ['QUEUE']], worker_threads=12, prefetch_multiplier=1)
	worker.start()

	running = True
	while running:
		if worker.consumer_stopped or worker.worker_stopped:
			running = False
			logger.info("Worker thread is not running anymore, stopping Worker.")
		else:
			time.sleep(1)

	worker.stop(5 * 1000)
	broker.emit_before("process_stop")
	broker.close()



print(threading.Thread(target=remoulade_thread, daemon=True).start())
print(threading.Thread(target=synchronization_thread, daemon=True).start())




version: '3.9'

services:

  caddy:
    image: caddy:2-alpine
    networks:
      - frontend
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./caddy/Caddyfile:/etc/caddy/Caddyfile
      - ./caddy/Caddyfile_auth:/etc/caddy/Caddyfile_auth
      - caddy_data:/data
      - caddy_config:/config
    depends_on:
      apache:
        condition: service_healthy


  apache:
    image: koo5/apache${PP}:latest
    networks:
      - frontend
      - backend
    hostnet_ports:
    # note that hostnet_ports are currently a no-op
      - 80
    deploy:
      replicas: 1
      restart_policy:  # https://docs.docker.com/compose/compose-file/compose-file-v3/
        condition: any
    volumes:
      #- ./../sources/apache/conf/ /usr/local/apache2/conf/
      #- ./../sources/apache/conf/httpd.conf /usr/local/apache2/conf/httpd.conf
      - "/etc/localtime:/etc/localtime:ro"
      - tmp:/usr/local/apache2/htdocs/tmp/
      - ./../sources/static/:/usr/local/apache2/htdocs/static/
    depends_on:
      frontend:
        condition: service_healthy


  frontend:
    image: koo5/frontend${PP}:latest
    environment:
      SECRETS_DIR: /run/secrets/
      RABBITMQ_URL:
      REMOULADE_PG_URI:
      REMOULADE_API:
      REDIS_HOST:
      AGRAPH_HOST:
      AGRAPH_PORT:
      SERVICES_URL:
      DOWNLOAD_BASTION_URL: # thus frontend shouldn't need ALL_PROXY 
      WATCHMEDO:
    #args: # todo: make this changeable from the script. It's still a question how non-dev requirements file would be managed in practice, one should probably start by copying the dev one
    #  REQUIREMENTS_FILE: requirements.txt
    volumes:
      - tmp:/app/server_root/tmp
      - "/etc/localtime:/etc/localtime:ro"
    networks:
      - backend
    hostnet_ports:
      - 7788
    depends_on:
      services:
        condition: service_healthy
      csharp-services:
        condition: service_healthy
      remoulade-api:
        condition: service_healthy
    deploy:
      placement:
        constraints: [node.role == manager]
      restart_policy:
        condition: any
    secrets:
      - AGRAPH_SUPER_USER
      - AGRAPH_SUPER_PASSWORD


  workers:
  
    # "workers" will be replaced by "worker" 
    
    image: koo5/workers${PP}:latest
    environment:
      DETERMINANCY_CHECKER__USE__ENFORCER: "true"
      SERVICES_URL:
      CSHARP_SERVICES_URL:
      RABBITMQ_URL:
      REMOULADE_PG_URI:
      REMOULADE_API:
      REDIS_HOST:
      AGRAPH_HOST:
      AGRAPH_PORT:
      WATCHMEDO:
    volumes:
      - tmp:/app/server_root/tmp
      - cache:/app/cache
      - ./../tests:/app/tests
      - "/etc/localtime:/etc/localtime:ro"
      # for gtrace:
      - "/tmp/.X11-unix:/tmp/.X11-unix:rw"
    networks:
      - backend
      - webproxy           
    depends_on:
      # workers don't have a good way to healthcheck. So instead of sequencing other containers after workers, we sequence workers after everything else. Theres a small risk of a deadlock if we run frontend with no parallelism (only makes sense for debugging) and somebody comes along and makes a request just before the healthcheck is about to make a request. And users running synchronous requests could experience timeouts when the stack is starting up. But in practice, we dont enable container sequencing unless debugging container startup problems.
      services:
        condition: service_healthy
      remoulade-api:
        condition: service_healthy
      frontend: 
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      postgres:
        condition: service_healthy
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
      restart_policy:
        condition: any
    secrets:
      - AGRAPH_SUPER_USER
      - AGRAPH_SUPER_PASSWORD



  # manager:
    #   image: koo5/manager${PP}:latest
    #   environment:
    #     WATCHMEDO:
    #   volumes:
    #     - tmp:/app/server_root/tmp
    #     - "/etc/localtime:/etc/localtime:ro"
    #   networks:
    #     - backend
    #
    #   deploy:
    #     replicas: 1
    #     placement:
    #       constraints: [node.role == manager]
    #     restart_policy:
    #       condition: any
    #   depends_on:
    #     - remoulade-api
    #
    #      
    



#  worker:
#    image: koo5/worker${PP}:latest
#    environment:
#      WATCHMEDO:
#      ALL_PROXY:
#    volumes:
#      - tmp:/app/server_root/tmp
#      - "/etc/localtime:/etc/localtime:ro"
#    networks:
#      - backend
#      - webproxy           
#    depends_on:
#      - manager
#    deploy:
#      replicas: 1
#      placement:
#        constraints: [node.role == manager]
#      restart_policy:
#        condition: any
   


  services:
    image: koo5/services${PP}:latest
    environment:
      WATCHMEDO:
      ALL_PROXY:
    volumes:
      - tmp:/app/server_root/tmp
      - "/etc/localtime:/etc/localtime:ro"
    networks:
      - backend
      - webproxy           
    hostnet_ports:
      - 17788
    depends_on:
      - redis
      - agraph:
          condition: service_healthy
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
      restart_policy:
        condition: any


  csharp-services:
    image: koo5/csharp-services${PP}:latest
    volumes:
      - tmp:/app/server_root/tmp
      - "/etc/localtime:/etc/localtime:ro"
      - "../sources/static/:/App/data/"
      - "/home/myuser/.dotnet"
    networks:
      - backend
    hostnet_ports:
      - 17789
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
      restart_policy:
        condition: any
    environment:
      CSHARPSERVICES_DATADIR: "/App/data/"
        


  redis:
    image: redis:6-alpine
    networks:
      - backend
    hostnet_ports:
      - 6379
    deploy:
      replicas: 1
      restart_policy:
        condition: any
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
    volumes:
      - "redis_data:/data"
    command: '/usr/local/bin/docker-entrypoint.sh --appendonly yes' # --save 60 1



  agraph:
    image: koo5/agraph${PP}:latest
    networks:
    - backend
    - frontend
    hostnet_ports:
      - 10035
    deploy:
      replicas: 1
      restart_policy:
        condition: any
        delay: "240s"
    secrets:
      - AGRAPH_SUPER_USER
      - AGRAPH_SUPER_PASSWORD
    environment:
      AGRAPH_SUPER_PASSWORD_FILE: /run/secrets/AGRAPH_SUPER_PASSWORD
      AGRAPH_SUPER_USER_FILE: /run/secrets/AGRAPH_SUPER_USER
    volumes:
      - "/etc/localtime:/etc/localtime:ro"
      - "agdata:/agraph/data"
      - "agconfig:/agraph/etc"
      - type: tmpfs
        target: /dev/shm
        tmpfs:
           size: 2096000000
    #shm_size: "1G" # not supported in swarm
    #finishme: use low-memory configuration


  rabbitmq:
    image: rabbitmq:management
    networks:
      - backend
    hostnet_ports:
      - 5672
    deploy:
      restart_policy:
        condition: any
    volumes:
      - "/etc/localtime:/etc/localtime:ro"
      - rabbitmq:/var/lib/rabbitmq
      - "${PWD}/rabbitmq/stuff.conf:/etc/rabbitmq/conf.d/30-stuff.conf"

    healthcheck:
        test: rabbitmq-diagnostics -q status && rabbitmq-diagnostics -q check_local_alarms


  postgres:
    image: bitnami/postgresql:15.1.0
    networks:
      - backend
    hostnet_ports:
      - 5433
    volumes:
      - 'postgresql_data:/bitnami/postgresql'
#    command: ["postgres", "-c", "max_connections=2000", "-c", "shared_buffers=2048MB"]
    environment:
      POSTGRESQL_EXTRA_FLAGS: -c max_connections=2000 -c shared_buffers=2048MB -c port=5433
      ALLOW_EMPTY_PASSWORD: "yes"
      POSTGRESQL_POSTGRES_PASSWORD: ""
      POSTGRESQL_DATABASE: remoulade
      POSTGRESQL_USERNAME: remoulade
    deploy:
      replicas: 1
      restart_policy:
        condition: any
    healthcheck:
      test: ["CMD-SHELL", "pg_isready --dbname=remoulade --username=remoulade --timeout=100 --port=5433 --host=127.0.0.1"]
      interval: 30s
      timeout: 5s
      retries: 5



  superbowl:
    image: koo5/super-bowl:latest
    networks:
      - backend
    hostnet_ports:
      - 1238
    environment:
      SUPERBOWL_WEBSERVER_PORT: 1238
      REMOULADE_BACKEND_LOCATION: http://127.0.0.1:5005
      WATCHMEDO:
    deploy:
      replicas: 1
      restart_policy:
        condition: any
    depends_on:
      remoulade-api:
        condition: service_healthy


  remoulade-api:
    image: koo5/remoulade-api${PP}:latest
    networks:
      - backend
    hostnet_ports:
      - 5005
    environment:
      SECRETS_DIR: /run/secrets/
      REMOULADE_PG_URI:
      RABBITMQ_URL:
      REDIS_HOST:
      AGRAPH_HOST:
      AGRAPH_PORT:
      FLASK_DEBUG:
      FLASK_ENV:
      WATCHMEDO:
    deploy:
      replicas: 1
      restart_policy:
        condition: any
    volumes:
      - "/etc/localtime:/etc/localtime:ro"
    depends_on:
      postgres:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
    secrets:
      - AGRAPH_SUPER_USER
      - AGRAPH_SUPER_PASSWORD


  download: #bastion
    image: koo5/download${PP}:latest
    environment:
      WATCHMEDO:
      ALL_PROXY: # still useful for, say, caching, right? not sure if download bastion is useful for anything though.
    volumes:
      - tmp:/app/server_root/tmp
      - "/etc/localtime:/etc/localtime:ro"
    networks:
      - backend
      - webproxy           
    hostnet_ports:
      - 6457
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
      restart_policy:
        condition: any


  webproxy:
    image: distahl/squid 
    #^ needs soources
    environment:
       PROXY_UID: 13
       PROXY_GID: 13
    volumes:
      - "${PWD}/squid/etc/:/etc/squid"
    hostnet_ports:
      - 3128
      - 3129
    networks:
      - webproxy           

# ^
# docker run \
#       --name squid \
#       -e TZ=Europe/London \
#       -e PROXY_UID=13 \
#       -e PROXY_GID=13 \
#       -v (pwd)/squid/etc/:/etc/squid \
#       -v (pwd)/squid/logs/:/var/log/ \
#       -p 3128:3128 \
#       -p 3129:3129 \
#       -it --rm distahl/squid

#  webproxy2:
#    image: ubuntu/squid 
#    environment:
#       TZ: UTC
#    volumes:
#      - "${PWD}/squid/etc/squid.conf:/etc/squid/squid.conf"
#      - squid_cache:/var/cache/squid
#    hostnet_ports:
#      - 3128
#    networks:
#      - webproxy           

# ^
# docker run --rm -it --name squid -e TZ=UTC -p 3128:3128 -v (pwd)/squid/etc/:/etc/squid/  -v sss:/var/spool/squid ubuntu/squid:5.2-22.04_beta
# this works, but how to mount cache dir without permission errors?
# 1704298510.574    101 172.17.0.1 TCP_DENIED/403 3865 CONNECT www.root.cz:443 - HIER_NONE/- text/html

# ^  # but we'll need to roll our own to eventually copy the configs in, so it can run in the cloud.

# ALL_PROXY=localhost:3128 curl https://www.root.cz/



networks:
  frontend:
    attachable: true
  backend:
    #driver: overlay
    attachable: true
  hostnet:
    external: true
    name: host
  webproxy:
             

volumes:
  tmp:
  cache:
  agdata:
  agconfig:
  rabbitmq:
  redis_data:
  caddy_data:
  caddy_config:
  postgresql_data:
  squid_cache:

